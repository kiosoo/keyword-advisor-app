# -*- coding: utf-8 -*-
"""
C√îNG C·ª§ C·ªê V·∫§N C·ª¶A L√ù VƒÇN HI·ªÜP(KIOSOO)
X√¢y d·ª±ng b·∫±ng Streamlit.
"""
import streamlit as st
from pytrends.request import TrendReq
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import LinearRegression
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
import time
from requests.exceptions import ReadTimeout

# --- D·ªØ li·ªáu tƒ©nh ---
COUNTRIES = {
    'Vietnam': 'VN', 'United States': 'US', 'Japan': 'JP', 'South Korea': 'KR',
    'United Kingdom': 'GB', 'Germany': 'DE', 'France': 'FR', 'Canada': 'CA',
    'Australia': 'AU', 'India': 'IN', 'Brazil': 'BR', 'Russia': 'RU',
    'Thailand': 'TH', 'Singapore': 'SG', 'Malaysia': 'MY',
    'Indonesia': 'ID', 'Philippines': 'PH', 'Taiwan': 'TW', 'Hong Kong': 'HK',
    'Spain': 'ES', 'Portugal': 'PT'
}
SORTED_COUNTRIES = dict(sorted(COUNTRIES.items()))

# --- C·∫•u h√¨nh trang web ---
st.set_page_config(page_title="C√îNG C·ª§ C·ªê V·∫§N C·ª¶A L√ù VƒÇN HI·ªÜP(KIOSOO)", page_icon="üß†", layout="wide")

# --- Kh·ªüi t·∫°o Session State ƒë·ªÉ qu·∫£n l√Ω API Key ---
if 'youtube_key_index' not in st.session_state:
    st.session_state.youtube_key_index = 0

# --- C√°c h√†m t√≠nh to√°n v√† ti·ªán √≠ch ---
# @st.cache_data(ttl=3600) # T·∫°m th·ªùi t·∫Øt cache ƒë·ªÉ ki·ªÉm tra l·ªói 429 d·ªÖ h∆°n, sau c√≥ th·ªÉ b·∫≠t l·∫°i
def analyze_trends_data(keywords, country_code, timeframe, gprop):
    """
    H√ÄM ƒê∆Ø·ª¢C N√ÇNG C·∫§P V·ªöI LOGIC T·ª∞ ƒê·ªòNG TH·ª¨ L·∫†I (EXPONENTIAL BACKOFF)
    """
    retries = 3  # S·ªë l·∫ßn th·ª≠ l·∫°i t·ªëi ƒëa
    delay = 1    # Th·ªùi gian ch·ªù ban ƒë·∫ßu (gi√¢y)
    
    for i in range(retries):
        try:
            # Th√™m timeout ƒë·ªÉ tr√°nh ·ª©ng d·ª•ng b·ªã treo
            pytrends = TrendReq(hl='en-US', tz=360, timeout=(10, 25)) 
            pytrends.build_payload(kw_list=keywords, timeframe=timeframe, geo=country_code, gprop=gprop)
            
            interest_df = pytrends.interest_over_time()
            related_queries = pytrends.related_queries()

            if interest_df.empty:
                return None, None, "Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu."
            
            interest_df.drop(columns=['isPartial'], inplace=True, errors='ignore')
            return interest_df, related_queries, None # Tr·∫£ v·ªÅ k·∫øt qu·∫£ n·∫øu th√†nh c√¥ng

        except ReadTimeout:
             if i < retries - 1:
                st.toast(f"Google Trends ph·∫£n h·ªìi ch·∫≠m, ƒëang th·ª≠ l·∫°i l·∫ßn {i+1}/{retries-1}...")
                time.sleep(delay)
                delay *= 2 # G·∫•p ƒë√¥i th·ªùi gian ch·ªù
             else:
                return None, None, "L·ªói: Google Trends kh√¥ng ph·∫£n h·ªìi k·ªãp th·ªùi. Vui l√≤ng th·ª≠ l·∫°i sau."

        except Exception as e:
            # Ch·ªâ th·ª≠ l·∫°i n·∫øu g·∫∑p l·ªói 429
            if 'response with code 429' in str(e):
                if i < retries - 1:
                    st.toast(f"B·ªã gi·ªõi h·∫°n truy c·∫≠p, ƒëang th·ª≠ l·∫°i sau {delay} gi√¢y...")
                    time.sleep(delay)
                    delay *= 2 # G·∫•p ƒë√¥i th·ªùi gian ch·ªù cho l·∫ßn th·ª≠ ti·∫øp theo
                else:
                    # N·∫øu th·ª≠ h·∫øt s·ªë l·∫ßn m√† v·∫´n l·ªói
                    return None, None, f"L·ªói: Google Trends ƒëang t·∫°m th·ªùi gi·ªõi h·∫°n truy c·∫≠p. Vui l√≤ng th·ª≠ l·∫°i sau √≠t ph√∫t. (Code: 429)"
            else:
                # N·∫øu l√† l·ªói kh√°c, b√°o l·ªói ngay l·∫≠p t·ª©c
                return None, None, f"ƒê√£ x·∫£y ra l·ªói kh√¥ng x√°c ƒë·ªãnh: {e}"
    
    return None, None, "ƒê√£ th·ª≠ l·∫°i nhi·ªÅu l·∫ßn nh∆∞ng kh√¥ng th√†nh c√¥ng."


def search_youtube_videos_with_rotation(query):
    api_keys_str = st.secrets.get("YOUTUBE_API_KEYS", "")
    if not api_keys_str:
        return None, "L·ªói c·∫•u h√¨nh: Vui l√≤ng th√™m YOUTUBE_API_KEYS v√†o Streamlit Secrets."
    
    api_keys = [key.strip() for key in api_keys_str.split(',')]
    start_index = st.session_state.youtube_key_index
    
    for i in range(len(api_keys)):
        current_key_index = (start_index + i) % len(api_keys)
        api_key = api_keys[current_key_index]
        
        try:
            youtube = build('youtube', 'v3', developerKey=api_key)
            request = youtube.search().list(q=query, part='snippet', maxResults=5, type='video', order='relevance')
            response = request.execute()
            st.session_state.youtube_key_index = (current_key_index + 1) % len(api_keys)
            videos = [{'id': item['id']['videoId'], 'title': item['snippet']['title'], 'channel': item['snippet']['channelTitle'], 'thumbnail': item['snippet']['thumbnails']['high']['url'], 'url': f'https://www.youtube.com/watch?v={item["id"]["videoId"]}'} for item in response.get('items', [])]
            return videos, None
        except HttpError as e:
            error_details = e.error_details
            is_quota_error = any(detail.get('reason') in ['quotaExceeded', 'dailyLimitExceeded'] for detail in error_details) if error_details else False
            if is_quota_error:
                print(f"Key {current_key_index + 1} ƒë√£ h·∫øt quota. ƒêang chuy·ªÉn sang key ti·∫øp theo...")
                continue
            else:
                return None, f"L·ªói API v·ªõi Key {current_key_index + 1}: {e}. Vui l√≤ng ki·ªÉm tra l·∫°i key."
        except Exception as e:
            return None, f"ƒê√£ x·∫£y ra l·ªói kh√¥ng x√°c ƒë·ªãnh: {e}"
    return None, "T·∫•t c·∫£ c√°c API key ƒë·ªÅu ƒë√£ h·∫øt h·∫°n ng·∫°ch ho·∫∑c kh√¥ng h·ª£p l·ªá."

def calculate_potential_score(interest_series, related_queries_data):
    if interest_series.empty or interest_series.sum() == 0:
        return {'score': 0, 'label': "Kh√¥ng c√≥ d·ªØ li·ªáu", 'color': "#6c757d"}
    avg_interest = interest_series.mean()
    avg_interest_score = min((avg_interest / 80), 1) * 30 
    X = np.arange(len(interest_series)).reshape(-1, 1)
    y = interest_series.values
    model = LinearRegression().fit(X, y)
    slope = model.coef_[0]
    normalized_slope = (slope / (avg_interest + 1e-6)) * 100
    trend_score = 10 + normalized_slope
    trend_score = np.clip(trend_score, 0, 20)
    interest_score = avg_interest_score + trend_score
    growth_score = 0
    rising_df = related_queries_data.get('rising')
    if rising_df is not None and not rising_df.empty:
        def process_rising_value(val):
            if isinstance(val, str) and 'Breakout' in val: return 10000
            numeric_val = pd.to_numeric(str(val).replace('+','').replace('%','').replace(',',''), errors='coerce')
            return numeric_val if pd.notnull(numeric_val) else 0
        rising_df['value_numeric'] = rising_df['value'].apply(process_rising_value)
        num_rising_score = min(len(rising_df) / 10, 1) * 25
        avg_growth_score = min(rising_df['value_numeric'].mean() / 1000, 1) * 25
        growth_score = num_rising_score + avg_growth_score
    total_score = int(interest_score + growth_score)
    if total_score >= 70: label, color = 'Ti·ªÅm nƒÉng cao', '#28a745'
    elif total_score >= 40: label, color = 'Trung b√¨nh', '#ffc107'
    else: label, color = 'B√£o h√≤a / C·∫°nh tranh cao', '#dc3545'
    return {'score': total_score, 'label': label, 'color': color, 'interest_score': interest_score, 'growth_score': growth_score, 'slope': slope, 'avg_interest': avg_interest, 'rising_df': rising_df}

def generate_advice(kw, metrics):
    score = metrics['score']
    full_advice = {
        "gold": f"**üü¢ ƒê√ÇY L√Ä M·ªòT C∆† H·ªòI V√ÄNG!** ...",
        "popular": f"**üü¢ CH·ª¶ ƒê·ªÄ ƒêANG R·∫§T TH·ªäNH H√ÄNH!** ...",
        "sustainable": f"**üü° C∆† H·ªòI NG√ÅCH B·ªÄN V·ªÆNG.** ...",
        "evergreen": f"**üü° CH·ª¶ ƒê·ªÄ 'EVERGREEN' C·∫¶N T√åM NG√ÅCH.** ...",
        "saturated": f"**üî¥ C·∫®N TR·ªåNG - TH·ªä TR∆Ø·ªúNG B√ÉO H√íA.** ...",
        "low_interest": f"**üî¥ CH·ª¶ ƒê·ªÄ √çT QUAN T√ÇM.** ..."
    } # Gi·ªØ ng·∫Øn g·ªçn ƒë·ªÉ t·∫≠p trung v√†o logic
    if score >= 70: return full_advice["gold"] if metrics.get('growth_score', 0) > metrics.get('interest_score', 0) else full_advice["popular"]
    elif score >= 40: return full_advice["sustainable"] if metrics.get('slope', 0) > 0 else full_advice["evergreen"]
    else: return full_advice["saturated"] if metrics.get('avg_interest', 0) > 30 else full_advice["low_interest"]

# --- Giao di·ªán ng∆∞·ªùi d√πng ---
st.title("üß† C√îNG C·ª§ C·ªê V·∫§N C·ª¶A L√ù VƒÇN HI·ªÜP(KIOSOO)")
st.markdown("Ph√¢n t√≠ch, ch·∫•m ƒëi·ªÉm v√† ƒë∆∞a ra l·ªùi khuy√™n chi·∫øn l∆∞·ª£c cho c√°c ch·ªß ƒë·ªÅ c·ªßa b·∫°n.")
st.sidebar.header("‚öôÔ∏è T√πy ch·ªçn Ph√¢n t√≠ch")
timeframe = st.sidebar.selectbox("1. Khung th·ªùi gian", [('7 ng√†y qua', 'now 7-d'), ('30 ng√†y qua', 'today 1-m'), ('90 ng√†y qua', 'today 3-m'), ('12 th√°ng qua', 'today 12-m'), ('5 nƒÉm qua', 'today 5-y'), ('T·ª´ 2004', 'all')], format_func=lambda x: x[0])[1]
gprop = st.sidebar.selectbox("2. N·ªÅn t·∫£ng t√¨m ki·∫øm", [('Web Search', ''), ('YouTube', 'youtube'), ('Google Images', 'images')], format_func=lambda x: x[0])[1]

with st.form("input_form"):
    col1, col2 = st.columns([3, 1])
    with col1: keywords_str = st.text_input("Nh·∫≠p c√°c t·ª´ kh√≥a (c√°ch nhau b·∫±ng d·∫•u ph·∫©y)", "n·ªìi chi√™n kh√¥ng d·∫ßu, m√°y √©p ch·∫≠m, m√°y l√†m s·ªØa h·∫°t")
    with col2: country_name = st.selectbox("Ch·ªçn qu·ªëc gia", options=list(SORTED_COUNTRIES.keys()), index=0); country_code = SORTED_COUNTRIES[country_name]
    submitted = st.form_submit_button("üí° Ph√¢n t√≠ch & T∆∞ v·∫•n")

if submitted:
    keywords = [kw.strip() for kw in keywords_str.split(',') if kw.strip()]
    if not keywords or not country_code: st.warning("Vui l√≤ng nh·∫≠p t·ª´ kh√≥a v√† ch·ªçn qu·ªëc gia.")
    else:
        with st.spinner("ƒêang ph√¢n t√≠ch v√† ch·∫•m ƒëi·ªÉm..."):
            interest_data, related_data, error = analyze_trends_data(keywords, country_code, timeframe, gprop)
        if error: st.error(error)
        elif interest_data is not None:
            st.header("1. B·∫£ng ƒëi·ªÉm Ti·ªÅm nƒÉng")
            # Code hi·ªÉn th·ªã gi·ªØ nguy√™n
            score_cards = st.columns(len(keywords)); all_metrics = {}
            for i, kw in enumerate(keywords):
                metrics = calculate_potential_score(interest_data.get(kw, pd.Series()), related_data.get(kw, {}))
                all_metrics[kw] = metrics
                with score_cards[i]: st.markdown(f"""<div style="background-color:{metrics['color']}; color:white; padding: 20px; border-radius: 10px; text-align: center; height: 180px; display: flex; flex-direction: column; justify-content: center;"><h4 style="color: white; margin-bottom: 5px;">{kw.upper()}</h4><h1 style="color: white; font-size: 3.5rem; margin: 0;">{metrics['score']}</h1><p style="color: white; margin-top: 5px;">{metrics['label']}</p></div>""", unsafe_allow_html=True)
            
            st.header("2. Ph√¢n t√≠ch & L·ªùi khuy√™n t·ª´ C·ªë v·∫•n")
            for kw in keywords:
                with st.expander(f"**Xem ph√¢n t√≠ch chi ti·∫øt cho t·ª´ kh√≥a: '{kw}'**"):
                    metrics = all_metrics[kw]
                    if 'avg_interest' in metrics:
                        advice = generate_advice(kw, metrics)
                        st.markdown(advice)
                    else:
                        st.info(f"Kh√¥ng c√≥ ƒë·ªß d·ªØ li·ªáu xu h∆∞·ªõng cho t·ª´ kh√≥a '{kw}' ƒë·ªÉ ƒë∆∞a ra l·ªùi khuy√™n chi ti·∫øt.")

            st.header("3. Bi·ªÉu ƒë·ªì so s√°nh M·ª©c ƒë·ªô quan t√¢m")
            # Code hi·ªÉn th·ªã gi·ªØ nguy√™n
            fig, ax = plt.subplots(figsize=(15, 7))
            for kw in keywords:
                if kw in interest_data.columns:
                    ax.plot(interest_data.index, interest_data[kw], label=kw)
            ax.set_title(f"So s√°nh xu h∆∞·ªõng t·∫°i '{country_name}'", fontsize=16); ax.legend(); ax.grid(True); st.pyplot(fig)

            st.header("4. D·ªØ li·ªáu chi ti·∫øt (Insight TƒÉng tr∆∞·ªüng)")
            # Code hi·ªÉn th·ªã gi·ªØ nguy√™n
            for kw in keywords:
                if kw in related_data and related_data[kw]:
                    with st.expander(f"**Xem Insight v√† Video h√†ng ƒë·∫ßu cho: '{kw}'**"):
                        tab1, tab2, tab3 = st.tabs(["üìä Truy v·∫•n H√†ng ƒë·∫ßu", "üìà Truy v·∫•n TƒÉng tr∆∞·ªüng", "üé¨ Videos YouTube"])
                        with tab1: top_df = related_data[kw].get('top'); st.markdown("##### **C√°c t·ª´ kh√≥a ƒë∆∞·ª£c t√¨m ki·∫øm nhi·ªÅu nh·∫•t li√™n quan**"); st.dataframe(top_df)
                        with tab2: rising_df = all_metrics[kw].get('rising_df'); st.markdown("##### **C√°c t·ª´ kh√≥a c√≥ m·ª©c tƒÉng tr∆∞·ªüng t√¨m ki·∫øm ƒë·ªôt ph√°**"); st.dataframe(rising_df)
                        with tab3:
                            st.markdown("##### **5 Video YouTube h√†ng ƒë·∫ßu cho t·ª´ kh√≥a li√™n quan**")
                            if top_df is not None and not top_df.empty:
                                top_query = top_df['query'].iloc[0]
                                with st.spinner(f"ƒêang t√¨m video cho '{top_query}'..."): videos, error = search_youtube_videos_with_rotation(top_query)
                                if error: st.error(error)
                                elif videos:
                                    for video in videos:
                                        v_col1, v_col2 = st.columns([1, 4]);
                                        with v_col1: st.image(video['thumbnail'])
                                        with v_col2: st.markdown(f"**[{video['title']}]({video['url']})**"); st.caption(f"K√™nh: {video['channel']}")
                                        st.markdown("---") 
                                else: st.info(f"Kh√¥ng t√¨m th·∫•y video n√†o cho t·ª´ kh√≥a '{top_query}'.")
                            else: st.info("Kh√¥ng c√≥ t·ª´ kh√≥a h√†ng ƒë·∫ßu ƒë·ªÉ t√¨m ki·∫øm video.")
```

---
### ## Gi·∫£i ph√°p 2: S·ª≠ d·ª•ng Proxy xoay v√≤ng (Rotating Proxies) - T·ªët nh·∫•t & Chuy√™n nghi·ªáp (C√≥ ph√≠)

ƒê√¢y l√† gi·∫£i ph√°p tri·ªát ƒë·ªÉ nh·∫•t, ƒë∆∞·ª£c s·ª≠ d·ª•ng b·ªüi c√°c h·ªá th·ªëng chuy√™n nghi·ªáp.

* **Logic:** Thay v√¨ t·∫•t c·∫£ y√™u c·∫ßu ƒë·ªÅu xu·∫•t ph√°t t·ª´ m·ªôt ƒë·ªãa ch·ªâ IP duy nh·∫•t c·ªßa m√°y ch·ªß Streamlit, ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng m·ªôt d·ªãch v·ª• proxy ƒë·ªÉ "che gi·∫•u" v√† thay ƒë·ªïi ƒë·ªãa ch·ªâ IP cho m·ªói y√™u c·∫ßu. T·ª´ g√≥c nh√¨n c·ªßa Google, c√°c y√™u c·∫ßu n√†y ƒë·∫øn t·ª´ h√†ng trƒÉm ƒë·ªãa ƒëi·ªÉm kh√°c nhau tr√™n th·∫ø gi·ªõi, khi·∫øn vi·ªác ph√°t hi·ªán v√† ch·∫∑n tr·ªü n√™n g·∫ßn nh∆∞ kh√¥ng th·ªÉ. 

* **∆Øu ƒëi·ªÉm:**
    * Hi·ªáu qu·∫£ g·∫ßn nh∆∞ 100% trong vi·ªác ch·ªëng l·ªói 429.
    * T·ªëc ƒë·ªô c√≥ th·ªÉ nhanh h∆°n v√¨ c√°c d·ªãch v·ª• proxy th∆∞·ªùng ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a.
    * L√† gi·∫£i ph√°p c√≥ th·ªÉ m·ªü r·ªông (scalable) khi ·ª©ng d·ª•ng c·ªßa b·∫°n c√≥ nhi·ªÅu ng∆∞·ªùi d√πng.

* **Nh∆∞·ª£c ƒëi·ªÉm:**
    * **C√≥ chi ph√≠.** B·∫°n s·∫Ω ph·∫£i tr·∫£ ti·ªÅn h√†ng th√°ng cho m·ªôt d·ªãch v·ª• cung c·∫•p proxy (v√≠ d·ª•: Bright Data, ScraperAPI, Smartproxy).

* **C√°ch tri·ªÉn khai (V√≠ d·ª•):**
    1.  ƒêƒÉng k√Ω m·ªôt d·ªãch v·ª• proxy v√† l·∫•y th√¥ng tin k·∫øt n·ªëi (host, port, username, password).
    2.  L∆∞u c√°c th√¥ng tin n√†y v√†o **Secrets** c·ªßa Streamlit.
    3.  C·∫•u h√¨nh `pytrends` ƒë·ªÉ s·ª≠ d·ª•ng proxy ƒë√≥.

    ```python
    # V√≠ d·ª• c√°ch c·∫•u h√¨nh pytrends v·ªõi proxy
    
    # L·∫•y th√¥ng tin t·ª´ secrets
    proxy_host = st.secrets["proxy"]["host"]
    proxy_port = st.secrets["proxy"]["port"]
    proxy_user = st.secrets["proxy"]["user"]
    proxy_pass = st.secrets["proxy"]["password"]

    # T·∫°o chu·ªói proxy
    proxy_url = f"http://{proxy_user}:{proxy_pass}@{proxy_host}:{proxy_port}"
    
    # T·∫°o dictionary proxies
    proxies = {
       'http': proxy_url,
       'https': proxy_url,
    }

    # Truy·ªÅn v√†o pytrends
    pytrends = TrendReq(hl='en-US', tz=360, requests_args={'proxies': proxies})
    

